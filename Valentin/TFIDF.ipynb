{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'writeandexecute' magic loaded.\n"
     ]
    }
   ],
   "source": [
    "#!pip install ipyext\n",
    "%load_ext ipyext.writeandexecute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writeandexecute -i user_accounts tfidf.py\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import tfidf\n",
    "from IPython.display import clear_output\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    no_punct = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return no_punct\n",
    "\n",
    "def stem_sentence(bag_of_word):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(word) for word in bag_of_word]\n",
    "\n",
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict\n",
    "\n",
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict\n",
    "\n",
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf\n",
    "\n",
    "def get_tfidf(textA, textB, max_words):\n",
    "    #documentA = remove_punctuation('the man went out for a walk')\n",
    "    #documentB = remove_punctuation('the children sat around the fire')\n",
    "    \n",
    "    #bagOfWordsA = tokenizer.tokenize(documentA)\n",
    "    #bagOfWordsB = tokenizer.tokenize(documentB)\n",
    "    \n",
    "    #uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))\n",
    "    \n",
    "    #numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
    "    #for word in bagOfWordsA:\n",
    "    #    numOfWordsA[word] += 1\n",
    "    \n",
    "    #numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
    "    #for word in bagOfWordsB:\n",
    "    #    numOfWordsB[word] += 1\n",
    "    \n",
    "    #tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
    "    #tfB = computeTF(numOfWordsB, bagOfWordsB)\n",
    "    \n",
    "    #idf = computeIDF([numOfWordsA, numOfWordsB])\n",
    "    \n",
    "    #tfidfA = computeTFIDF(tfA, idf)\n",
    "    #tfidfB = computeTFIDF(tfB, idf)\n",
    "    #df = pd.DataFrame([tfidfA, tfidfB])\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=max_words)\n",
    "    vectors = vectorizer.fit_transform([textA, textB])\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    \n",
    "    return pd.DataFrame(denselist, columns=feature_names)\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return [w for w in tokenizer.tokenize(text) if w.lower() not in stopwords.words(\"english\")]\n",
    "\n",
    "def clean_df(df):\n",
    "    count = 0\n",
    "    clean_text = []\n",
    "    for sentence in df[\"text\"]:\n",
    "        clean_text.append(remove_stopwords(sentence))\n",
    "        count+=1\n",
    "        clear_output()\n",
    "        print(f\"{count}/{len(df)}\")\n",
    "    df[\"text\"] = clean_text\n",
    "    df[\"text\"] = [\" \".join(stem_sentence(x)) for x in df[\"text\"]]\n",
    "    return df\n",
    "\n",
    "    \n",
    "def get_tfidf_words(df, max_words):\n",
    "    df = clean_df(df)\n",
    "    all_positive_texts = \" \".join([df[\"text\"][i] for i in range(len(df)) if df[\"label\"][i] == 1])\n",
    "    all_negative_texts = \" \".join([df[\"text\"][i] for i in range(len(df)) if df[\"label\"][i] == 0])\n",
    "    \n",
    "    extracted_words = []\n",
    "    for i in range(len(df)):\n",
    "        idf = []\n",
    "        if df[\"label\"][i] == 0:\n",
    "            idf = get_tfidf(df[\"text\"][i], all_positive_texts, max_words = max_words)\n",
    "        else :\n",
    "            idf = get_tfidf(df[\"text\"][i], all_negative_texts, max_words = max_words)\n",
    "        extracted_words.append(list(idf.keys()))\n",
    "    df[\"tfidf\"] = extracted_words\n",
    "    \n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
