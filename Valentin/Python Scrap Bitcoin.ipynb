{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "fromDate = \"2019-09-16\"\n",
    "toDate = \"2019-09-18\"\n",
    "URL = f\"https://bloskfnnnsqm.herokuapp.com/scrap?from={fromDate}&to={toDate}\"\n",
    "URLlocal = f\"http://localhost:8080/scrap?from={fromDate}&to={toDate}\"\n",
    "r = requests.get(url = URL) \n",
    "\n",
    "data = r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text = \"\".join(data).replace('\\xa0', \" \").replace(\".\", \" \")\n",
    "#text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-16_2019-09-18.txt\n"
     ]
    }
   ],
   "source": [
    "fileName = f\"{fromDate}_{toDate}.txt\"\n",
    "print(fileName)\n",
    "f=open(fileName, \"w+\")\n",
    "f.write(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Important words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from spacy.lang.fr import French\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "def getImportantWords(text):\n",
    "    nlp = French()\n",
    "    doc = nlp(text)\n",
    "    return list(filter(lambda x: x.is_stop==False, doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "importantWords = getImportantWords(text)\n",
    "#importantWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Let\\'s meet for a drink at the park of the golden head, side door of the Rhone children to discuss Crypto, Blockchain, meet members of the association and discuss the schedule of the season The event is open to all, no need to be an expert, we are here to exchange and share; eat, drink and play! Feel free to bring your favorite dish or drink to complete our makeshift buffet We will distribute stickers Bitcoin at the event to arrange Guerilla Marketing! Downturn in Meltdown under climate bug \"After the exchange decentralized holdhold which to buy and sell OTC for bitcoins via the Lightning Network, Sparkswap today launching its desktop application (Windows, MacOS and Linux) , for the exchange of US dollars against LN of bitcoins sparkswap This first version of Desktop is designed for experienced users who are already running NLD Lightning App Zap or Node Launcher Source: medium com / sparkswap \"As can be seen on the statistics site transactionfee information SegWit the percentage of transactions on the Bitcoin network has reached 50.05% precisely Since September 14, the rate remains above 50% if all transactions on blockchain went through BTC SegWit, the number of transactions per second theoretically see its capacity increased to 16 4 tx / s we can assess the current network capacity to 8 tx / s Article Rémy R to read in the Financial Markets CoinL\\'Autorité Journal recently published the testimony of two victims of scams presented as \"scams bitcoin\" or \"the cryptomonnaies\" \"I\\'ve realized that it was a scam going on a specialized site that publishes a list of fraudulent sites \"- Thank you for the reference to our list, unless either of the excellent work produced by the community CryptoFR'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "englishText = translator.translate(text, dest='en').text\n",
    "englishText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import treebank\n",
    "train_sents = treebank.tagged_sents()[:3000]\n",
    "tagger = UnigramTagger(train_sents)\n",
    "from nltk.corpus import sentiwordnet as swn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTextEmotions(text) :\n",
    "    words = text.split(\" \")\n",
    "    result_type_text = tagger.tag(words)\n",
    "    wordsToKeep = list(filter(lambda x : x[1] in [\"JJ\", \"JJR\", \"JJS\", \"RBR\", \"RBS\", \"RP\", \"NP\"], result_type_text ))\n",
    "    wordsFeelings = list(map(lambda x : swn.senti_synsets(x[0]), wordsToKeep))\n",
    "    \n",
    "    sommePos = 0\n",
    "    sommeNeg = 0\n",
    "    for wordFeelings in wordsFeelings :\n",
    "        pos = 0\n",
    "        neg = 0\n",
    "        for feelings in list(wordFeelings):\n",
    "            pos += feelings.pos_score()\n",
    "            neg += feelings.neg_score()\n",
    "        sommePos += pos\n",
    "        sommeNeg += neg\n",
    "        \n",
    "        highest = sommePos if sommePos > sommeNeg else sommeNeg\n",
    "    score = ((highest/(sommeNeg + sommePos)) * 2) - 1\n",
    "    \n",
    "    if highest == sommeNeg:\n",
    "        score = -score\n",
    "    print(f\"Pos :{sommePos}, Neg :{sommeNeg}, score : {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.125925925925926\n",
      "Pos :7.375, Neg :9.5, pc : -0.125925925925926\n"
     ]
    }
   ],
   "source": [
    "getTextEmotions(englishText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatizedText = []\n",
    "    words = nltk.word_tokenize(text)\n",
    "    wordTokens = nltk.pos_tag(words)\n",
    "    for wordToken in wordTokens:\n",
    "        wordType = wordToken[1]\n",
    "        word =  wordToken[0]\n",
    "        if wordType[0:2] == \"JJ\":\n",
    "            lemmatizedText.append(lemmatizer.lemmatize(word, \"a\"))\n",
    "        elif wordType[0] == \"V\":\n",
    "            lemmatizedText.append(lemmatizer.lemmatize(word, \"v\"))\n",
    "        else :\n",
    "            lemmatizedText.append(lemmatizer.lemmatize(word))\n",
    "    return \" \".join(lemmatizedText)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i keep look good best golden rock corpora'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(\"i kept looking better best golden rocks corpora\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
