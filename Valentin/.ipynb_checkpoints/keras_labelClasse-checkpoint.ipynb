{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras_bitcoin\n",
    "import dataset\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import permutations, combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files loaded :  300\n",
      "Counter({1: 50803, 0: 38379})\n",
      "                                                text  label        date  \\\n",
      "0                                            covort!      1  2017-01-01   \n",
      "1  Oh, come on, guys, this is a funny comment - u...      1  2017-01-01   \n",
      "2  That keeps the spam out! Screw the 3rd world!\\...      1  2017-01-01   \n",
      "3      They look like the cuck men from buzzfeed lol      1  2017-01-01   \n",
      "4  that is the most retarded quote i have ever read.      1  2017-01-01   \n",
      "\n",
      "   score  nb_replies  stickied  \n",
      "0     14           1     False  \n",
      "1      3           0     False  \n",
      "2     17           0     False  \n",
      "3    -11           0     False  \n",
      "4     -7           1     False  \n"
     ]
    }
   ],
   "source": [
    "#df = pd.read_csv(\"../Data/LDA_Data/save3.csv\")\n",
    "df = dataset.get_labeled_dataset(number_of_file=300, from_date = \"2017-01-01\")\n",
    "print(collections.Counter(df[\"label\"]))\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    21885\n",
       "5    17918\n",
       "3    17461\n",
       "2    16542\n",
       "1    15376\n",
       "Name: labelClasse, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_labeled_bitcoin_price_classe(df):\n",
    "    df_bitcoin_values = pd.read_csv(\"../Data/Bitcoin_Price/chart_price_BTC.csv\")\n",
    "    \n",
    "    length = len(df_bitcoin_values)\n",
    "    bictoin_price_dict = {} \n",
    "\n",
    "    for index in range(1,length-1):\n",
    "        value = float(df_bitcoin_values[\"priceBTC\"][index+1]) - float(df_bitcoin_values[\"priceBTC\"][index])\n",
    "        pourcentage = 100 * value / float(df_bitcoin_values[\"priceBTC\"][index - 1])\n",
    "        result = 0\n",
    "        if pourcentage >5 :\n",
    "            result = 5\n",
    "        elif pourcentage <-5:\n",
    "            result = 1\n",
    "        elif pourcentage <-1.2:\n",
    "            result = 2\n",
    "        elif pourcentage <1.2:\n",
    "            result = 3\n",
    "        else :\n",
    "            result = 4\n",
    "        bictoin_price_dict[df_bitcoin_values[\"dateMidnight\"][index][0:10]] = result\n",
    "    labelClasse=[]\n",
    "    for index, row in df.iterrows():\n",
    "        if row[\"date\"] in bictoin_price_dict.keys() :\n",
    "            labelClasse.append(bictoin_price_dict[row[\"date\"]])\n",
    "    df[\"labelClasse\"] = labelClasse\n",
    "    #df = df.drop(columns=[\"label\"])\n",
    "    return df\n",
    "df = get_labeled_bitcoin_price_classe(df)\n",
    "df[\"labelClasse\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    33992\n",
       "1    30348\n",
       "4    10945\n",
       "3     7287\n",
       "2     6610\n",
       "Name: labelClasse, dtype: int64"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"labelClasse\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        14\n",
       "1         3\n",
       "2        17\n",
       "3       -11\n",
       "4        -7\n",
       "5         0\n",
       "6         1\n",
       "7         2\n",
       "8         4\n",
       "9         2\n",
       "10        3\n",
       "11        3\n",
       "12       11\n",
       "13       53\n",
       "14        7\n",
       "15        3\n",
       "16        2\n",
       "17        4\n",
       "18        3\n",
       "19        3\n",
       "20        7\n",
       "21        1\n",
       "22        0\n",
       "23       16\n",
       "24       -3\n",
       "25        1\n",
       "26        7\n",
       "27        1\n",
       "28        1\n",
       "29        1\n",
       "         ..\n",
       "89152     1\n",
       "89153     1\n",
       "89154     1\n",
       "89155     1\n",
       "89156     1\n",
       "89157     1\n",
       "89158     2\n",
       "89159     3\n",
       "89160     1\n",
       "89161     1\n",
       "89162     1\n",
       "89163     1\n",
       "89164     0\n",
       "89165     1\n",
       "89166     2\n",
       "89167     1\n",
       "89168     1\n",
       "89169     1\n",
       "89170     1\n",
       "89171     1\n",
       "89172     1\n",
       "89173     0\n",
       "89174     1\n",
       "89175     1\n",
       "89176     1\n",
       "89177     2\n",
       "89178     0\n",
       "89179     1\n",
       "89180     1\n",
       "89181     0\n",
       "Name: score, Length: 89182, dtype: int64"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89182\n",
      "40432\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "textColumnName = \"text\"\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w.lower() not in stopwords.words(\"english\")]\n",
    "    return words\n",
    "\n",
    "import string\n",
    "def remove_punctuation(text):\n",
    "    no_punct = \"\".join([c.lower() for c in text if c not in string.punctuation])\n",
    "    return no_punct\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "def stem_sentence(sentence_array):\n",
    "    return list(map(lambda word: porter.stem(word),sentence_array))\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "#df[textColumnName] = [remove_punctuation(x) for x in df[textColumnName]]\n",
    "df[textColumnName] = [tknzr.tokenize(x) for x in df[textColumnName]]\n",
    "#df[textColumnName] = [remove_stopwords(x) for x in df[textColumnName]]\n",
    "df[textColumnName] = [stem_sentence(x) for x in df[textColumnName]]\n",
    "df[textColumnName] = [\" \".join(x) for x in df[textColumnName]]\n",
    "print(len(df[textColumnName]))\n",
    "df = df[df[textColumnName].apply(lambda x: len(x) > 35)]\n",
    "df = df[df[\"score\"].apply(lambda x: x > 1)]\n",
    "print(len(df[textColumnName]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17357, 7)\n",
      "                                                text  label        date  \\\n",
      "1  Oh , come on , guy , thi is a funni comment - ...      1  2017-01-01   \n",
      "2  that keep the spam out ! screw the 3rd world !...      1  2017-01-01   \n",
      "3        they look like the cuck men from buzzfe lol      1  2017-01-01   \n",
      "4    that is the most retard quot i have ever read .      1  2017-01-01   \n",
      "5  https://twitter.com/rogerkver/status/807223816...      1  2017-01-01   \n",
      "\n",
      "   score  nb_replies  stickied  labelClasse  \n",
      "1      3           0     False            5  \n",
      "2     17           0     False            5  \n",
      "3    -11           0     False            5  \n",
      "4     -7           1     False            5  \n",
      "5      0           1     False            5  \n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_hub in d:\\programmes\\anacondapython\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in d:\\programmes\\anacondapython\\lib\\site-packages (from tensorflow_hub) (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in d:\\programmes\\anacondapython\\lib\\site-packages (from tensorflow_hub) (1.16.4)\n",
      "Requirement already satisfied: six>=1.10.0 in d:\\programmes\\anacondapython\\lib\\site-packages (from tensorflow_hub) (1.12.0)\n",
      "Requirement already satisfied: setuptools in d:\\programmes\\anacondapython\\lib\\site-packages (from protobuf>=3.4.0->tensorflow_hub) (41.0.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.0.0\n",
      "Eager mode:  True\n",
      "Hub version:  0.7.0\n",
      "GPU is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>date</th>\n",
       "      <th>score</th>\n",
       "      <th>nb_replies</th>\n",
       "      <th>stickied</th>\n",
       "      <th>labelClasse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oh , come on , guy , thi is a funni comment - ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that keep the spam out ! screw the 3rd world !...</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>let' not allow blockstream to buy cheap coin u...</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&gt; In the entir histori of the world , noth tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>obligatori ... thi is gentlemen . iab 4 ... \" ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label        date  \\\n",
       "1   Oh , come on , guy , thi is a funni comment - ...      1  2017-01-01   \n",
       "2   that keep the spam out ! screw the 3rd world !...      1  2017-01-01   \n",
       "7   let' not allow blockstream to buy cheap coin u...      1  2017-01-01   \n",
       "8   > In the entir histori of the world , noth tha...      1  2017-01-01   \n",
       "10  obligatori ... thi is gentlemen . iab 4 ... \" ...      1  2017-01-01   \n",
       "\n",
       "    score  nb_replies  stickied  labelClasse  \n",
       "1       3           0     False            4  \n",
       "2      17           0     False            4  \n",
       "7       2           0     False            4  \n",
       "8       4           3     False            4  \n",
       "10      3           0     False            4  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PredictionEvalData(model, data) :\n",
    "    X_Eval = tf.convert_to_tensor(data, dtype= tf.string)\n",
    "    res = model.predict(X_Eval)\n",
    "    Y_pred = []\n",
    "    for note in res :\n",
    "    #print(note)\n",
    "        Y_pred.append(note.argmax())\n",
    "    return Y_pred;\n",
    "\n",
    "def RealisationDuModel(reviews, activation, activation2, activation3, activation4) :\n",
    "    # Séparation Train Test et Validation\n",
    "    train = reviews[0:int(len(reviews)*0.8)]\n",
    "    test = reviews[int(len(reviews)*0.8)+1:]\n",
    "    train_study, train_study_test = train_test_split(train, test_size = 0.2, random_state=0)\n",
    "    # Séparation des X et Y (on enlève 1 au Y pour que cela fonctionne)\n",
    "    X_train = train_study[\"text\"]\n",
    "    Y_train = train_study[\"labelClasse\"] - 1\n",
    "    X_train_test = train_study_test[\"text\"]\n",
    "    Y_train_test = train_study_test[\"labelClasse\"] -1\n",
    "    X_test = test[\"text\"]\n",
    "    Y_test = test[\"labelClasse\"]-1\n",
    "    # Conversion en tenseur pour pouvoir être utilisé dans tensorflow :\n",
    "    X_train = tf.convert_to_tensor(X_train, dtype= tf.string)\n",
    "    Y_train = tf.convert_to_tensor(Y_train, dtype= tf.int32)\n",
    "    X_train_test = tf.convert_to_tensor(X_train_test, dtype= tf.string)\n",
    "    Y_train_test = tf.convert_to_tensor(Y_train_test, dtype= tf.int32)\n",
    "    X_test = tf.convert_to_tensor(X_test, dtype= tf.string)\n",
    "    Y_test = tf.convert_to_tensor(Y_test, dtype= tf.int32)\n",
    "    ### Construction du modèle :\n",
    "    # Construction de l'embedding :\n",
    "    embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "    hub_layer = hub.KerasLayer(embedding, output_shape=[20], input_shape=[], \n",
    "                            dtype=tf.string, trainable=True)\n",
    "    # Construction des couches du modèle : \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(hub_layer)\n",
    "    model.add(tf.keras.layers.Dense(40 ,activation=activation))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    model.add(tf.keras.layers.Dense(20 ,activation=activation2))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    model.add(tf.keras.layers.Dense(10 ,activation=activation3))\n",
    "    model.add(tf.keras.layers.Dense(5, activation=activation4))\n",
    "    # Construction du Compile :\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['mse'])\n",
    "    #model.summary()\n",
    "    # Entrainement de notre modèle sur les données de train :\n",
    "    print(\"Entrainement du modèle\")\n",
    "    history = model.fit(X_train,\n",
    "                    Y_train,\n",
    "                    epochs=9, validation_data=(X_train_test,Y_train_test),\n",
    "                    verbose=0)\n",
    "    # Evaluation du modèle :\n",
    "    eval_predicted = PredictionEvalData(model, X_test)\n",
    "    dif = []\n",
    "    for i in range(0,len(eval_predicted)):\n",
    "            dif.append(eval_predicted[i] - Y_test[i])\n",
    "    # Retourne le modèle :\n",
    "    return dif,eval_predicted,Y_test, test, mean_squared_error(eval_predicted,Y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrainement du modèle\n",
      "3.04176267281106 tanh tanh tanh tanh\n",
      "Entrainement du modèle\n",
      "2.625 tanh tanh tanh softmax\n",
      "Entrainement du modèle\n",
      "6.481854838709677 tanh tanh tanh softplus\n",
      "Entrainement du modèle\n",
      "4.755472350230415 tanh tanh tanh sigmoid\n",
      "Entrainement du modèle\n",
      "5.1396889400921655 tanh tanh tanh hard_sigmoid\n",
      "Entrainement du modèle\n",
      "3.411290322580645 tanh tanh tanh exponential\n",
      "Entrainement du modèle\n",
      "4.047235023041475 tanh tanh tanh linear\n",
      "Entrainement du modèle\n",
      "4.972062211981567 tanh tanh softmax tanh\n",
      "Entrainement du modèle\n",
      "2.625 tanh tanh softmax softmax\n",
      "Entrainement du modèle\n",
      "5.568548387096774 tanh tanh softmax softplus\n",
      "Entrainement du modèle\n",
      "2.9922235023041477 tanh tanh softmax sigmoid\n",
      "Entrainement du modèle\n",
      "5.1074308755760365 tanh tanh softmax hard_sigmoid\n",
      "Entrainement du modèle\n",
      "3.539746543778802 tanh tanh softmax exponential\n",
      "Entrainement du modèle\n",
      "3.040898617511521 tanh tanh softmax linear\n",
      "Entrainement du modèle\n",
      "4.985599078341014 tanh tanh softplus tanh\n",
      "Entrainement du modèle\n",
      "4.356566820276497 tanh tanh softplus softmax\n",
      "Entrainement du modèle\n",
      "7.2393433179723505 tanh tanh softplus softplus\n",
      "Entrainement du modèle\n",
      "3.740495391705069 tanh tanh softplus sigmoid\n",
      "Entrainement du modèle\n",
      "3.1811635944700463 tanh tanh softplus hard_sigmoid\n",
      "Entrainement du modèle\n",
      "4.36463133640553 tanh tanh softplus exponential\n",
      "Entrainement du modèle\n",
      "7.748271889400922 tanh tanh softplus linear\n",
      "Entrainement du modèle\n",
      "7.216589861751152 tanh tanh sigmoid tanh\n",
      "Entrainement du modèle\n",
      "2.625 tanh tanh sigmoid softmax\n",
      "Entrainement du modèle\n",
      "4.690668202764977 tanh tanh sigmoid softplus\n",
      "Entrainement du modèle\n",
      "2.8202764976958523 tanh tanh sigmoid sigmoid\n",
      "Entrainement du modèle\n",
      "5.559907834101383 tanh tanh sigmoid hard_sigmoid\n",
      "Entrainement du modèle\n",
      "4.023041474654378 tanh tanh sigmoid exponential\n",
      "Entrainement du modèle\n",
      "2.623847926267281 tanh tanh sigmoid linear\n",
      "Entrainement du modèle\n",
      "4.5625 tanh tanh hard_sigmoid tanh\n",
      "Entrainement du modèle\n",
      "4.356566820276497 tanh tanh hard_sigmoid softmax\n",
      "Entrainement du modèle\n",
      "5.535714285714286 tanh tanh hard_sigmoid softplus\n",
      "Entrainement du modèle\n",
      "2.9639976958525347 tanh tanh hard_sigmoid sigmoid\n",
      "Entrainement du modèle\n",
      "5.118951612903226 tanh tanh hard_sigmoid hard_sigmoid\n",
      "Entrainement du modèle\n",
      "7.944988479262673 tanh tanh hard_sigmoid exponential\n",
      "Entrainement du modèle\n",
      "5.318548387096774 tanh tanh hard_sigmoid linear\n",
      "Entrainement du modèle\n",
      "7.905241935483871 tanh tanh exponential tanh\n",
      "Entrainement du modèle\n",
      "8.088133640552995 tanh tanh exponential softmax\n",
      "Entrainement du modèle\n",
      "2.970910138248848 tanh tanh exponential softplus\n",
      "Entrainement du modèle\n",
      "8.02102534562212 tanh tanh exponential sigmoid\n",
      "Entrainement du modèle\n",
      "3.9740783410138247 tanh tanh exponential hard_sigmoid\n",
      "Entrainement du modèle\n",
      "5.108870967741935 tanh tanh exponential exponential\n",
      "Entrainement du modèle\n",
      "3.8110599078341014 tanh tanh exponential linear\n",
      "Entrainement du modèle\n",
      "4.780241935483871 tanh tanh linear tanh\n",
      "Entrainement du modèle\n",
      "2.625 tanh tanh linear softmax\n",
      "Entrainement du modèle\n",
      "5.270161290322581 tanh tanh linear softplus\n",
      "Entrainement du modèle\n",
      "5.117799539170507 tanh tanh linear sigmoid\n",
      "Entrainement du modèle\n",
      "4.084389400921659 tanh tanh linear hard_sigmoid\n",
      "Entrainement du modèle\n",
      "4.089573732718894 tanh tanh linear exponential\n",
      "Entrainement du modèle\n",
      "4.061635944700461 tanh tanh linear linear\n",
      "Entrainement du modèle\n",
      "2.642857142857143 tanh softmax tanh tanh\n",
      "Entrainement du modèle\n",
      "2.625 tanh softmax tanh softmax\n",
      "Entrainement du modèle\n",
      "4.354550691244239 tanh softmax tanh softplus\n",
      "Entrainement du modèle\n",
      "4.267281105990784 tanh softmax tanh sigmoid\n",
      "Entrainement du modèle\n",
      "4.350518433179723 tanh softmax tanh hard_sigmoid\n",
      "Entrainement du modèle\n",
      "5.150921658986175 tanh softmax tanh exponential\n",
      "Entrainement du modèle\n",
      "4.065380184331797 tanh softmax tanh linear\n",
      "Entrainement du modèle\n",
      "4.220046082949309 tanh softmax softmax tanh\n",
      "Entrainement du modèle\n",
      "8.088133640552995 tanh softmax softmax softmax\n",
      "Entrainement du modèle\n",
      "3.0892857142857144 tanh softmax softmax softplus\n",
      "Entrainement du modèle\n",
      "8.079493087557603 tanh softmax softmax sigmoid\n",
      "Entrainement du modèle\n",
      "8.070852534562212 tanh softmax softmax hard_sigmoid\n",
      "Entrainement du modèle\n",
      "4.067684331797235 tanh softmax softmax exponential\n",
      "Entrainement du modèle\n",
      "2.625 tanh softmax softmax linear\n",
      "Entrainement du modèle\n",
      "2.8934331797235022 tanh softmax softplus tanh\n",
      "Entrainement du modèle\n",
      "2.625 tanh softmax softplus softmax\n",
      "Entrainement du modèle\n",
      "4.8896889400921655 tanh softmax softplus softplus\n",
      "Entrainement du modèle\n",
      "6.926267281105991 tanh softmax softplus sigmoid\n",
      "Entrainement du modèle\n",
      "2.631912442396313 tanh softmax softplus hard_sigmoid\n",
      "Entrainement du modèle\n",
      "5.493663594470046 tanh softmax softplus exponential\n",
      "Entrainement du modèle\n",
      "2.8934331797235022 tanh softmax softplus linear\n",
      "Entrainement du modèle\n",
      "8.088133640552995 tanh softmax sigmoid tanh\n",
      "Entrainement du modèle\n",
      "4.356566820276497 tanh softmax sigmoid softmax\n",
      "Entrainement du modèle\n",
      "3.330357142857143 tanh softmax sigmoid softplus\n",
      "Entrainement du modèle\n",
      "2.7701612903225805 tanh softmax sigmoid sigmoid\n",
      "Entrainement du modèle\n",
      "4.3703917050691246 tanh softmax sigmoid hard_sigmoid\n",
      "Entrainement du modèle\n",
      "6.1618663594470044 tanh softmax sigmoid exponential\n",
      "Entrainement du modèle\n",
      "4.9501728110599075 tanh softmax sigmoid linear\n",
      "Entrainement du modèle\n",
      "4.356566820276497 tanh softmax hard_sigmoid tanh\n",
      "Entrainement du modèle\n",
      "8.088133640552995 tanh softmax hard_sigmoid softmax\n",
      "Entrainement du modèle\n",
      "6.144873271889401 tanh softmax hard_sigmoid softplus\n",
      "Entrainement du modèle\n",
      "3.3617511520737327 tanh softmax hard_sigmoid sigmoid\n",
      "Entrainement du modèle\n",
      "4.255760368663594 tanh softmax hard_sigmoid hard_sigmoid\n",
      "Entrainement du modèle\n",
      "2.883352534562212 tanh softmax hard_sigmoid exponential\n",
      "Entrainement du modèle\n",
      "4.356566820276497 tanh softmax hard_sigmoid linear\n",
      "Entrainement du modèle\n",
      "5.1618663594470044 tanh softmax exponential tanh\n",
      "Entrainement du modèle\n",
      "8.088133640552995 tanh softmax exponential softmax\n",
      "Entrainement du modèle\n",
      "2.922811059907834 tanh softmax exponential softplus\n",
      "Entrainement du modèle\n",
      "4.329205069124424 tanh softmax exponential sigmoid\n",
      "Entrainement du modèle\n",
      "3.5172811059907834 tanh softmax exponential hard_sigmoid\n",
      "Entrainement du modèle\n",
      "2.625864055299539 tanh softmax exponential exponential\n",
      "Entrainement du modèle\n",
      "2.8934331797235022 tanh softmax exponential linear\n",
      "Entrainement du modèle\n",
      "5.015552995391705 tanh softmax linear tanh\n",
      "Entrainement du modèle\n",
      "8.088133640552995 tanh softmax linear softmax\n",
      "Entrainement du modèle\n",
      "4.381912442396313 tanh softmax linear softplus\n",
      "Entrainement du modèle\n",
      "3.415898617511521 tanh softmax linear sigmoid\n",
      "Entrainement du modèle\n",
      "4.351958525345622 tanh softmax linear hard_sigmoid\n",
      "Entrainement du modèle\n",
      "8.066532258064516 tanh softmax linear exponential\n",
      "Entrainement du modèle\n",
      "8.014400921658986 tanh softmax linear linear\n",
      "Entrainement du modèle\n",
      "5.15639400921659 tanh softplus tanh tanh\n",
      "Entrainement du modèle\n",
      "2.8934331797235022 tanh softplus tanh softmax\n",
      "Entrainement du modèle\n",
      "2.6169354838709675 tanh softplus tanh softplus\n",
      "Entrainement du modèle\n",
      "2.704205069124424 tanh softplus tanh sigmoid\n",
      "Entrainement du modèle\n",
      "4.866935483870968 tanh softplus tanh hard_sigmoid\n",
      "Entrainement du modèle\n",
      "7.680299539170507 tanh softplus tanh exponential\n",
      "Entrainement du modèle\n",
      "3.963709677419355 tanh softplus tanh linear\n",
      "Entrainement du modèle\n",
      "3.1431451612903225 tanh softplus softmax tanh\n",
      "Entrainement du modèle\n",
      "2.8934331797235022 tanh softplus softmax softmax\n",
      "Entrainement du modèle\n",
      "7.985023041474655 tanh softplus softmax softplus\n",
      "Entrainement du modèle\n",
      "7.233294930875576 tanh softplus softmax sigmoid\n",
      "Entrainement du modèle\n",
      "2.7255184331797233 tanh softplus softmax hard_sigmoid\n",
      "Entrainement du modèle\n",
      "5.573444700460829 tanh softplus softmax exponential\n",
      "Entrainement du modèle\n",
      "7.825748847926267 tanh softplus softmax linear\n",
      "Entrainement du modèle\n",
      "2.9732142857142856 tanh softplus softplus tanh\n",
      "Entrainement du modèle\n",
      "8.088133640552995 tanh softplus softplus softmax\n",
      "Entrainement du modèle\n",
      "3.478398617511521 tanh softplus softplus softplus\n",
      "Entrainement du modèle\n",
      "5.278801843317972 tanh softplus softplus sigmoid\n",
      "Entrainement du modèle\n",
      "4.504320276497696 tanh softplus softplus hard_sigmoid\n",
      "Entrainement du modèle\n",
      "4.662442396313364 tanh softplus softplus exponential\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrainement du modèle\n",
      "2.625 tanh softplus softplus linear\n",
      "Entrainement du modèle\n",
      "3.1117511520737327 tanh softplus sigmoid tanh\n",
      "Entrainement du modèle\n",
      "2.625 tanh softplus sigmoid softmax\n",
      "Entrainement du modèle\n",
      "4.151785714285714 tanh softplus sigmoid softplus\n",
      "Entrainement du modèle\n",
      "2.918778801843318 tanh softplus sigmoid sigmoid\n",
      "Entrainement du modèle\n",
      "6.151497695852535 tanh softplus sigmoid hard_sigmoid\n",
      "Entrainement du modèle\n",
      "3.9115783410138247 tanh softplus sigmoid exponential\n",
      "Entrainement du modèle\n",
      "4.356566820276497 tanh softplus sigmoid linear\n",
      "Entrainement du modèle\n",
      "5.133352534562212 tanh softplus hard_sigmoid tanh\n",
      "Entrainement du modèle\n",
      "2.625 tanh softplus hard_sigmoid softmax\n",
      "Entrainement du modèle\n",
      "8.014976958525345 tanh softplus hard_sigmoid softplus\n",
      "Entrainement du modèle\n",
      "4.193260368663594 tanh softplus hard_sigmoid sigmoid\n",
      "Entrainement du modèle\n",
      "4.448444700460829 tanh softplus hard_sigmoid hard_sigmoid\n",
      "Entrainement du modèle\n",
      "5.152361751152074 tanh softplus hard_sigmoid exponential\n",
      "Entrainement du modèle\n",
      "2.6287442396313363 tanh softplus hard_sigmoid linear\n",
      "Entrainement du modèle\n",
      "2.9694700460829493 tanh softplus exponential tanh\n",
      "Entrainement du modèle\n",
      "8.088133640552995 tanh softplus exponential softmax\n",
      "Entrainement du modèle\n",
      "4.031970046082949 tanh softplus exponential softplus\n",
      "Entrainement du modèle\n",
      "4.178571428571429 tanh softplus exponential sigmoid\n",
      "Entrainement du modèle\n",
      "8.088133640552995 tanh softplus exponential hard_sigmoid\n",
      "Entrainement du modèle\n",
      "5.147465437788019 tanh softplus exponential exponential\n",
      "Entrainement du modèle\n",
      "4.085253456221198 tanh softplus exponential linear\n",
      "Entrainement du modèle\n",
      "4.356566820276497 tanh softplus linear tanh\n",
      "Entrainement du modèle\n",
      "8.088133640552995 tanh softplus linear softmax\n",
      "Entrainement du modèle\n",
      "3.384504608294931 tanh softplus linear softplus\n",
      "Entrainement du modèle\n",
      "2.920794930875576 tanh softplus linear sigmoid\n",
      "Entrainement du modèle\n",
      "8.08208525345622 tanh softplus linear hard_sigmoid\n",
      "Entrainement du modèle\n",
      "5.6875 tanh softplus linear exponential\n",
      "Entrainement du modèle\n",
      "4.826036866359447 tanh softplus linear linear\n",
      "Entrainement du modèle\n",
      "2.6330645161290325 tanh sigmoid tanh tanh\n",
      "Entrainement du modèle\n",
      "8.088133640552995 tanh sigmoid tanh softmax\n",
      "Entrainement du modèle\n",
      "5.189516129032258 tanh sigmoid tanh softplus\n",
      "Entrainement du modèle\n",
      "3.4651497695852536 tanh sigmoid tanh sigmoid\n",
      "Entrainement du modèle\n",
      "4.402649769585254 tanh sigmoid tanh hard_sigmoid\n",
      "Entrainement du modèle\n",
      "3.811635944700461 tanh sigmoid tanh exponential\n",
      "Entrainement du modèle\n",
      "3.478398617511521 tanh sigmoid tanh linear\n",
      "Entrainement du modèle\n",
      "2.8934331797235022 tanh sigmoid softmax tanh\n",
      "Entrainement du modèle\n",
      "2.625 tanh sigmoid softmax softmax\n",
      "Entrainement du modèle\n",
      "4.35397465437788 tanh sigmoid softmax softplus\n",
      "Entrainement du modèle\n",
      "3.605126728110599 tanh sigmoid softmax sigmoid\n",
      "Entrainement du modèle\n",
      "4.355990783410138 tanh sigmoid softmax hard_sigmoid\n",
      "Entrainement du modèle\n",
      "2.890841013824885 tanh sigmoid softmax exponential\n",
      "Entrainement du modèle\n",
      "5.159562211981567 tanh sigmoid softmax linear\n",
      "Entrainement du modèle\n",
      "3.552995391705069 tanh sigmoid softplus tanh\n",
      "Entrainement du modèle\n",
      "8.088133640552995 tanh sigmoid softplus softmax\n",
      "Entrainement du modèle\n",
      "3.359158986175115 tanh sigmoid softplus softplus\n",
      "Entrainement du modèle\n",
      "5.555587557603687 tanh sigmoid softplus sigmoid\n",
      "Entrainement du modèle\n",
      "5.148329493087558 tanh sigmoid softplus hard_sigmoid\n",
      "Entrainement du modèle\n",
      "4.74020737327189 tanh sigmoid softplus exponential\n",
      "Entrainement du modèle\n",
      "3.054147465437788 tanh sigmoid softplus linear\n",
      "Entrainement du modèle\n",
      "2.867799539170507 tanh sigmoid sigmoid tanh\n",
      "Entrainement du modèle\n",
      "2.8934331797235022 tanh sigmoid sigmoid softmax\n",
      "Entrainement du modèle\n",
      "3.8695276497695854 tanh sigmoid sigmoid softplus\n",
      "Entrainement du modèle\n",
      "4.280529953917051 tanh sigmoid sigmoid sigmoid\n",
      "Entrainement du modèle\n",
      "5.48991935483871 tanh sigmoid sigmoid hard_sigmoid\n",
      "Entrainement du modèle\n",
      "3.0394585253456223 tanh sigmoid sigmoid exponential\n",
      "Entrainement du modèle\n",
      "4.356566820276497 tanh sigmoid sigmoid linear\n",
      "Entrainement du modèle\n",
      "8.088133640552995 tanh sigmoid hard_sigmoid tanh\n",
      "Entrainement du modèle\n",
      "8.088133640552995 tanh sigmoid hard_sigmoid softmax\n",
      "Entrainement du modèle\n",
      "4.630760368663594 tanh sigmoid hard_sigmoid softplus\n",
      "Entrainement du modèle\n",
      "4.859735023041475 tanh sigmoid hard_sigmoid sigmoid\n",
      "Entrainement du modèle\n",
      "7.140841013824885 tanh sigmoid hard_sigmoid hard_sigmoid\n",
      "Entrainement du modèle\n",
      "3.5924539170506913 tanh sigmoid hard_sigmoid exponential\n",
      "Entrainement du modèle\n",
      "2.637672811059908 tanh sigmoid hard_sigmoid linear\n",
      "Entrainement du modèle\n",
      "8.09360599078341 tanh sigmoid exponential tanh\n",
      "Entrainement du modèle\n",
      "8.088133640552995 tanh sigmoid exponential softmax\n",
      "Entrainement du modèle\n",
      "8.061347926267281 tanh sigmoid exponential softplus\n",
      "Entrainement du modèle\n",
      "7.4913594470046085 tanh sigmoid exponential sigmoid\n",
      "Entrainement du modèle\n",
      "4.631048387096774 tanh sigmoid exponential hard_sigmoid\n",
      "Entrainement du modèle\n",
      "4.503456221198157 tanh sigmoid exponential exponential\n",
      "Entrainement du modèle\n",
      "8.088133640552995 tanh sigmoid exponential linear\n",
      "Entrainement du modèle\n",
      "3.749135944700461 tanh sigmoid linear tanh\n",
      "Entrainement du modèle\n",
      "2.625 tanh sigmoid linear softmax\n",
      "Entrainement du modèle\n",
      "2.9703341013824884 tanh sigmoid linear softplus\n",
      "Entrainement du modèle\n",
      "5.067972350230415 tanh sigmoid linear sigmoid\n",
      "Entrainement du modèle\n",
      "5.122983870967742 tanh sigmoid linear hard_sigmoid\n",
      "Entrainement du modèle\n",
      "3.7828341013824884 tanh sigmoid linear exponential\n",
      "Entrainement du modèle\n",
      "4.386808755760368 tanh sigmoid linear linear\n",
      "Entrainement du modèle\n",
      "2.7309907834101383 tanh hard_sigmoid tanh tanh\n",
      "Entrainement du modèle\n",
      "8.088133640552995 tanh hard_sigmoid tanh softmax\n",
      "Entrainement du modèle\n",
      "7.349654377880184 tanh hard_sigmoid tanh softplus\n",
      "Entrainement du modèle\n",
      "4.400345622119816 tanh hard_sigmoid tanh sigmoid\n",
      "Entrainement du modèle\n",
      "2.734447004608295 tanh hard_sigmoid tanh hard_sigmoid\n",
      "Entrainement du modèle\n",
      "3.734158986175115 tanh hard_sigmoid tanh exponential\n",
      "Entrainement du modèle\n",
      "4.369815668202765 tanh hard_sigmoid tanh linear\n",
      "Entrainement du modèle\n",
      "2.8934331797235022 tanh hard_sigmoid softmax tanh\n",
      "Entrainement du modèle\n",
      "2.625 tanh hard_sigmoid softmax softmax\n",
      "Entrainement du modèle\n",
      "4.263824884792626 tanh hard_sigmoid softmax softplus\n",
      "Entrainement du modèle\n",
      "4.403513824884793 tanh hard_sigmoid softmax sigmoid\n",
      "Entrainement du modèle\n",
      "4.662154377880184 tanh hard_sigmoid softmax hard_sigmoid\n",
      "Entrainement du modèle\n",
      "2.908410138248848 tanh hard_sigmoid softmax exponential\n",
      "Entrainement du modèle\n",
      "4.356566820276497 tanh hard_sigmoid softmax linear\n",
      "Entrainement du modèle\n",
      "3.5043202764976957 tanh hard_sigmoid softplus tanh\n",
      "Entrainement du modèle\n",
      "2.625 tanh hard_sigmoid softplus softmax\n",
      "Entrainement du modèle\n",
      "4.93721198156682 tanh hard_sigmoid softplus softplus\n",
      "Entrainement du modèle\n",
      "2.756048387096774 tanh hard_sigmoid softplus sigmoid\n",
      "Entrainement du modèle\n",
      "8.09331797235023 tanh hard_sigmoid softplus hard_sigmoid\n",
      "Entrainement du modèle\n",
      "4.339861751152074 tanh hard_sigmoid softplus exponential\n",
      "Entrainement du modèle\n",
      "7.909274193548387 tanh hard_sigmoid softplus linear\n",
      "Entrainement du modèle\n",
      "2.8934331797235022 tanh hard_sigmoid sigmoid tanh\n",
      "Entrainement du modèle\n",
      "8.088133640552995 tanh hard_sigmoid sigmoid softmax\n",
      "Entrainement du modèle\n",
      "4.951900921658986 tanh hard_sigmoid sigmoid softplus\n",
      "Entrainement du modèle\n",
      "5.823444700460829 tanh hard_sigmoid sigmoid sigmoid\n",
      "Entrainement du modèle\n",
      "5.436059907834101 tanh hard_sigmoid sigmoid hard_sigmoid\n",
      "Entrainement du modèle\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-242-b03e9dded948>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mactivation3\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscrambled_list\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mactivation4\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscrambled_list\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                 \u001b[0mdifference\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRealisationDuModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m                 \u001b[0mafficher\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mactivation2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mactivation3\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mactivation4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mafficher\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-241-50aff07f9860>\u001b[0m in \u001b[0;36mRealisationDuModel\u001b[1;34m(reviews, activation, activation2, activation3, activation4)\u001b[0m\n\u001b[0;32m     48\u001b[0m                     \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m                     verbose=0)\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[1;31m# Evaluation du modèle :\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0meval_predicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPredictionEvalData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programmes\\AnacondaPython\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\Programmes\\AnacondaPython\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programmes\\AnacondaPython\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programmes\\AnacondaPython\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programmes\\AnacondaPython\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programmes\\AnacondaPython\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    485\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programmes\\AnacondaPython\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programmes\\AnacondaPython\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programmes\\AnacondaPython\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programmes\\AnacondaPython\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mD:\\Programmes\\AnacondaPython\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scrambled_list = [ 'tanh', 'softmax' ,'softplus', 'sigmoid', 'hard_sigmoid', 'exponential','linear', 'relu']\n",
    "res = pd.DataFrame(columns=('evaluation', 'activation1', 'activation2', 'activation3', 'activation4'))\n",
    "for activation in scrambled_list :\n",
    "    for activation2 in scrambled_list :\n",
    "        for activation3 in scrambled_list :\n",
    "            for activation4 in scrambled_list :\n",
    "                difference, pred, Y_t, test, evaluation = RealisationDuModel(df, activation, activation2, activation3, activation4)\n",
    "                afficher = str(evaluation) + \" \" + activation + \" \" + activation2 + \" \" + activation3 + \" \" + activation4\n",
    "                print(afficher)\n",
    "                res = res.append({'evaluation': evaluation, 'activation1' : activation, 'activation2' : activation2, 'activation3' : activation3, 'activation4' : activation4}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrainement du modèle\n"
     ]
    }
   ],
   "source": [
    "difference, pred, Y_t, test, evaluation = RealisationDuModel(df, 'tanh', 'softplus', 'tanh', 'softplus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6111798169675984"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evaluation</th>\n",
       "      <th>activation1</th>\n",
       "      <th>activation2</th>\n",
       "      <th>activation3</th>\n",
       "      <th>activation4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.625000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>tanh</td>\n",
       "      <td>tanh</td>\n",
       "      <td>softmax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.625000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>tanh</td>\n",
       "      <td>softmax</td>\n",
       "      <td>softmax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.625000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>softmax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.623848</td>\n",
       "      <td>tanh</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>linear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2.625000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>tanh</td>\n",
       "      <td>linear</td>\n",
       "      <td>softmax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2.625000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>softmax</td>\n",
       "      <td>tanh</td>\n",
       "      <td>softmax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2.625000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>softmax</td>\n",
       "      <td>softmax</td>\n",
       "      <td>linear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2.625000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>softmax</td>\n",
       "      <td>softplus</td>\n",
       "      <td>softmax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2.625864</td>\n",
       "      <td>tanh</td>\n",
       "      <td>softmax</td>\n",
       "      <td>exponential</td>\n",
       "      <td>exponential</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2.616935</td>\n",
       "      <td>tanh</td>\n",
       "      <td>softplus</td>\n",
       "      <td>tanh</td>\n",
       "      <td>softplus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>2.625000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>softplus</td>\n",
       "      <td>softplus</td>\n",
       "      <td>linear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2.625000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>softplus</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>softmax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2.625000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>softplus</td>\n",
       "      <td>hard_sigmoid</td>\n",
       "      <td>softmax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2.628744</td>\n",
       "      <td>tanh</td>\n",
       "      <td>softplus</td>\n",
       "      <td>hard_sigmoid</td>\n",
       "      <td>linear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>2.625000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>softmax</td>\n",
       "      <td>softmax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>2.625000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>linear</td>\n",
       "      <td>softmax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>2.625000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>hard_sigmoid</td>\n",
       "      <td>softmax</td>\n",
       "      <td>softmax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>2.625000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>hard_sigmoid</td>\n",
       "      <td>softplus</td>\n",
       "      <td>softmax</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     evaluation activation1   activation2   activation3  activation4\n",
       "1      2.625000        tanh          tanh          tanh      softmax\n",
       "8      2.625000        tanh          tanh       softmax      softmax\n",
       "22     2.625000        tanh          tanh       sigmoid      softmax\n",
       "27     2.623848        tanh          tanh       sigmoid       linear\n",
       "43     2.625000        tanh          tanh        linear      softmax\n",
       "50     2.625000        tanh       softmax          tanh      softmax\n",
       "62     2.625000        tanh       softmax       softmax       linear\n",
       "64     2.625000        tanh       softmax      softplus      softmax\n",
       "89     2.625864        tanh       softmax   exponential  exponential\n",
       "100    2.616935        tanh      softplus          tanh     softplus\n",
       "118    2.625000        tanh      softplus      softplus       linear\n",
       "120    2.625000        tanh      softplus       sigmoid      softmax\n",
       "127    2.625000        tanh      softplus  hard_sigmoid      softmax\n",
       "132    2.628744        tanh      softplus  hard_sigmoid       linear\n",
       "155    2.625000        tanh       sigmoid       softmax      softmax\n",
       "190    2.625000        tanh       sigmoid        linear      softmax\n",
       "204    2.625000        tanh  hard_sigmoid       softmax      softmax\n",
       "211    2.625000        tanh  hard_sigmoid      softplus      softmax"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2 = res[res[\"evaluation\"].apply(lambda x: x < 2.63)]\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "dif = []\n",
    "for i in range(0,len(pred)):\n",
    "    if Y_t[i]==4:\n",
    "        dif.append(pred[i] - Y_t[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffe = list(dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-e5c085804b5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'difference'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'labelClasse'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pred'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"labelClasse\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'difference'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-e5c085804b5c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'difference'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'labelClasse'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pred'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"labelClasse\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'difference'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "r = pd.DataFrame()\n",
    "r[\"date\"] = test[\"date\"]\n",
    "r[\"labelClasse\"] = test[\"labelClasse\"]\n",
    "r[\"pred\"] = pred\n",
    "r = r.groupby(['date']).max()\n",
    "r['difference'] = r['labelClasse'] - r['pred']\n",
    "print(r['difference'].value_counts())\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4, -3, -2, -1, 0, 1, 2, 3, 4]\n",
      "3472\n",
      "[1.0656682027649769, 3.6578341013824884, 11.376728110599078, 19.153225806451612, 20.362903225806452, 14.688940092165899, 13.997695852534562, 11.751152073732719, 3.945852534562212]\n"
     ]
    }
   ],
   "source": [
    "df_difference = pd.DataFrame(difference)\n",
    "df_difference.head()\n",
    "df_difference.columns=[\"dif\"]\n",
    "df_difference.head()\n",
    "dict_result = df_difference[\"dif\"].value_counts().sort_index()\n",
    "difPrediction = dict_result.index.tolist()\n",
    "print(dict_result.index.tolist())\n",
    "print(sum(dict_result))\n",
    "pourcentageRes = []\n",
    "for i in dict_result : \n",
    "    pourcentageRes.append(100 * i/sum(dict_result))\n",
    "print(pourcentageRes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pos = np.arange(len(difPrediction))\n",
    " \n",
    "# Create bars\n",
    "plt.bar(y_pos, pourcentageRes)\n",
    " \n",
    "# Create names on the x-axis\n",
    "plt.xticks(y_pos, difPrediction)\n",
    " \n",
    "# Show graphic\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6116392970325553\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(pred,Y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
