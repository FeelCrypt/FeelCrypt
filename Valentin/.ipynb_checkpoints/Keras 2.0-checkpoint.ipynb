{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_words_count(texts):\n",
    "    words_set = set()\n",
    "    for sentence in texts:\n",
    "        tokenize_word = word_tokenize(sentence)\n",
    "        for word in tokenize_word:\n",
    "            words_set.add(word)\n",
    "    return len(words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_above_multiple(num, divisor):\n",
    "    return math.ceil(num / divisor) * divisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(texts, labels):\n",
    "    vocab_length = get_above_multiple(get_unique_words_count(texts), 10)\n",
    "    coded_sentences = [one_hot(sentence, vocab_length) for sentence in texts]\n",
    "    max_sentence_size = max(list(map(lambda sentence : len(word_tokenize(sentence)), texts)))\n",
    "    padded_coded_sentences = pad_sequences(coded_sentences, max_sentence_size, padding='post') \n",
    "    texts_train, texts_test , labels_train, labels_test = train_test_split(padded_coded_sentences, labels , test_size = 0.20)\n",
    "    return texts_train,texts_test,labels_train,labels_test,vocab_length,max_sentence_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(texts_train, labels_train, vocab_length, max_sentence_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_length, 20, input_length=max_sentence_size))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    model.fit(texts_train, labels_train, epochs=100, verbose=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_to_predict(texts, vocab_length, max_sentence_size):\n",
    "    valid_sentences = list(filter(lambda sentence : len(word_tokenize(sentence)) <= max_sentence_size, texts))\n",
    "    valid_coded_sentences = list(map(lambda sentence : one_hot(sentence, vocab_length), valid_sentences))\n",
    "    \n",
    "    return pad_sequences(valid_coded_sentences, max_sentence_size, padding='post'),valid_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(texts, model, vocab_length, max_sentence_size):\n",
    "    to_predict, valid_texts = get_data_to_predict(texts, vocab_length, max_sentence_size)\n",
    "    predictions_values = model.predict(to_predict)\n",
    "    predictions_rounded = [round(pred[0]) for pred in predictions_values]\n",
    "    predictions = {}\n",
    "    for i in range(len(texts) -1):\n",
    "        predictions[valid_texts[i]] = predictions_rounded[i]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Create texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A) Text 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    # Positive Reviews\n",
    "\n",
    "    'This is an excellent movie',\n",
    "    'The move was fantastic I like it',\n",
    "    'You should watch it is brilliant',\n",
    "    'Exceptionally good',\n",
    "    'Wonderfully directed and executed I like it',\n",
    "    'Its a fantastic series',\n",
    "    'Never watched such a brillent movie',\n",
    "    'It is a Wonderful movie',\n",
    "\n",
    "    # Negtive Reviews\n",
    "\n",
    "    \"horrible acting\",\n",
    "    'waste of money',\n",
    "    'pathetic picture',\n",
    "    'It was very boring',\n",
    "    'I did not like the movie',\n",
    "    'The movie was horrible',\n",
    "    'I will not recommend',\n",
    "    'The acting is pathetic'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = array([1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B) Text 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Status\n",
       "0  A very, very, very slow-moving, aimless movie ...       0"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('imdb_labelled.txt', delimiter = '\\t', engine='python', quoting = 3)\n",
    "docs = df['Review']\n",
    "labels = array(df['Status'])\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\33777\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.6907 - acc: 0.3333\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.6844 - acc: 0.3333\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.6790 - acc: 0.3333\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 332us/step - loss: 0.6733 - acc: 0.4167\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 333us/step - loss: 0.6674 - acc: 0.4167\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 334us/step - loss: 0.6611 - acc: 0.4167\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.6548 - acc: 0.5000\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 334us/step - loss: 0.6485 - acc: 0.5000\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.6422 - acc: 0.5000\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 334us/step - loss: 0.6358 - acc: 0.5000\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.6294 - acc: 0.5000\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.6230 - acc: 0.5000\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 333us/step - loss: 0.6166 - acc: 0.5833\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 167us/step - loss: 0.6101 - acc: 0.5833\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.6036 - acc: 0.5833\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.5970 - acc: 0.5833\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 334us/step - loss: 0.5904 - acc: 0.5833\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 334us/step - loss: 0.5837 - acc: 0.5833\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 167us/step - loss: 0.5770 - acc: 0.6667\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.5702 - acc: 0.6667\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.5633 - acc: 0.6667\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.5563 - acc: 0.6667\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 253us/step - loss: 0.5493 - acc: 0.6667\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.5422 - acc: 0.6667\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 417us/step - loss: 0.5351 - acc: 0.6667\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.5278 - acc: 0.6667\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.5205 - acc: 0.6667\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 417us/step - loss: 0.5131 - acc: 0.6667\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.5057 - acc: 0.6667\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.4981 - acc: 0.6667\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.4905 - acc: 0.6667\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 417us/step - loss: 0.4827 - acc: 0.7500\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.4749 - acc: 0.7500\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.4669 - acc: 0.7500\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.4588 - acc: 0.7500\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.4507 - acc: 0.7500\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.4425 - acc: 0.7500\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 167us/step - loss: 0.4341 - acc: 0.7500\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 333us/step - loss: 0.4257 - acc: 0.7500\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 329us/step - loss: 0.4172 - acc: 0.7500\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 333us/step - loss: 0.4086 - acc: 0.7500\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.3998 - acc: 0.7500\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.3910 - acc: 0.7500\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.3820 - acc: 0.7500\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.3730 - acc: 0.7500\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.3639 - acc: 0.7500\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 167us/step - loss: 0.3546 - acc: 0.7500\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.3453 - acc: 0.7500\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.3359 - acc: 0.7500\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.3263 - acc: 0.7500\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.3167 - acc: 0.7500\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.3069 - acc: 0.7500\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 333us/step - loss: 0.2970 - acc: 0.7500\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.2870 - acc: 0.7500\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 333us/step - loss: 0.2770 - acc: 0.7500\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.2669 - acc: 0.7500\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.2566 - acc: 0.7500\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.2463 - acc: 0.7500\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.2359 - acc: 0.7500\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 167us/step - loss: 0.2255 - acc: 0.7500\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.2149 - acc: 0.7500\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 334us/step - loss: 0.2043 - acc: 0.7500\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.1935 - acc: 0.7500\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.1827 - acc: 0.7500\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.1718 - acc: 0.7500\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.1608 - acc: 0.7500\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 167us/step - loss: 0.1498 - acc: 0.7500\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.1386 - acc: 0.7500\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.1274 - acc: 0.7500\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 334us/step - loss: 0.1161 - acc: 0.7500\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 417us/step - loss: 0.1048 - acc: 0.7500\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 333us/step - loss: 0.0933 - acc: 0.7500\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.0818 - acc: 0.7500\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 417us/step - loss: 0.0702 - acc: 0.7500\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.0585 - acc: 0.7500\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: 0.0468 - acc: 0.7500\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 333us/step - loss: 0.0350 - acc: 0.7500\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 166us/step - loss: 0.0231 - acc: 0.7500\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 333us/step - loss: 0.0112 - acc: 0.7500\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 167us/step - loss: -8.6462e-04 - acc: 0.7500\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: -0.0129 - acc: 0.7500\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 167us/step - loss: -0.0250 - acc: 0.7500\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 168us/step - loss: -0.0372 - acc: 0.7500\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 251us/step - loss: -0.0494 - acc: 0.7500\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 333us/step - loss: -0.0617 - acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: -0.0740 - acc: 0.7500\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 251us/step - loss: -0.0864 - acc: 0.7500\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: -0.0989 - acc: 0.7500\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: -0.1114 - acc: 0.7500\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: -0.1240 - acc: 0.7500\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: -0.1366 - acc: 0.7500\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 251us/step - loss: -0.1493 - acc: 0.7500\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 167us/step - loss: -0.1620 - acc: 0.7500\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 167us/step - loss: -0.1748 - acc: 0.7500\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: -0.1877 - acc: 0.7500\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: -0.2006 - acc: 0.7500\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: -0.2135 - acc: 0.7500\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: -0.2265 - acc: 0.7500\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 167us/step - loss: -0.2396 - acc: 0.7500\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 250us/step - loss: -0.2527 - acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "#texts_train, texts_test , labels_train, labels_test, vocab_length, max_sentence_size = get_train_test_data(docs, labels)\n",
    "texts_train, texts_test , labels_train, labels_test, vocab_length, max_sentence_size = get_train_test_data(corpus, sentiments)\n",
    "model = get_model(texts_train, labels_train, vocab_length, max_sentence_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = [\n",
    "    \"excellent movie\",\n",
    "    \"i hated that movie\",\n",
    "    \"this is a great movie and the actor were really good, and it almost made me cry. That was awesome\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.51316816]\n",
      " [0.33023047]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'excellent movie': 1.0, 'i hated that movie': 0.0}"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predictions(new_text, model, vocab_length, max_sentence_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 44ms/step\n",
      "Accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(texts_test, labels_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2019-10-01': 0, '2019-10-02': 1, '2019-10-03': 1, '2019-10-04': 1, '2019-10-05': 1, '2019-10-06': 1, '2019-10-07': 0, '2019-10-08': 1, '2019-10-09': 1, '2019-10-10': 1, '2019-10-11': 0, '2019-10-12': 0, '2019-10-13': 0, '2019-10-14': 0, '2019-10-15': 0, '2019-10-16': 1, '2019-10-17': 1, '2019-10-18': 1, '2019-10-19': 1, '2019-10-20': 1, '2019-10-21': 1, '2019-10-22': 1, '2019-10-23': 1, '2019-10-24': 1, '2019-10-25': 1, '2019-10-26': 1, '2019-10-27': 1, '2019-10-28': 0}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "f = open(\"bitcoin_value.json\", \"r\")\n",
    "bitcoin_values = json.loads(f.read())\n",
    "print(bitcoin_values)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "for key in bitcoin_values:\n",
    "    labels.append(bitcoin_values[key])\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rencontre mensuelle organisée par Maxime, Thibault et Lola. Pas de présentation, mais juste un verre pour discuter de l’actualité des cryptomonnaies. C’est aussi l’occasion pour ceux qui débutent de venir poser leurs questions. « Combien de bitcoins distribuait le premier faucet bitcoin ? Quelle est la plus petite unité utilisable actuellement ? En 2010, un bug créa accidentellement une énorme quantité de bitcoin. Combien exactement ? Quel est le pseudo Bitcointalk de l’auteur de la célèbre expression HODL ? » Voilà quelques-unes des questions du LNquiz lancé le 6 mars 2019 par Jazaronaut sur Twitter avec la simple ambition de promouvoir le Lightning Network.  Suite à une donation, les récompenses du LNQuiz seront doublées durant deux semaines : 2000 satoshis pour chaque gagnant.\n",
      "\n",
      "« Combien de bitcoins distribuait le premier faucet bitcoin ? Quelle est la plus petite unité utilisable actuellement ? En 2010, un bug créa accidentellement une énorme quantité de bitcoin. Combien exactement ? Quel est le pseudo Bitcointalk de l’auteur de la célèbre expression HODL ? » Voilà quelques-unes des questions du LNquiz lancé le 6 mars 2019 par Jazaronaut sur Twitter avec la simple ambition de promouvoir le Lightning Network.  Suite à une donation, les récompenses du LNQuiz seront doublées durant deux semaines : 2000 satoshis pour chaque gagnant. Rencontre mensuelle organisée par Maxime, Thibault et Lola. Pas de présentation, mais juste un verre pour discuter de l’actualité des cryptomonnaies. C’est aussi l’occasion pour ceux qui débutent de venir poser leurs questions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "date = \"2019-10-\"\n",
    "for i in range(1, 28):\n",
    "    day = f\"{i}\"\n",
    "    if(len(day) < 2):\n",
    "        day = f\"0{day}\"\n",
    "    print(day)\n",
    "    f = open(f\"./Articles/{date}{day}.json\")\n",
    "    jsonData = json.loads(f.read())\n",
    "    f.close()\n",
    "    texts.append(\" \".join(jsonData[\"data\"]))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
