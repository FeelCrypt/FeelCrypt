{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1) Configuration de PRAW (Python Reddit Api Wrapper)\n",
    "## !! A faire avant de pouvoir utiliser n'importe laquelle des parties suivantes !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw               # API pour reddit\n",
    "import pandas as pd       # pour afficher les dictionnaires sous forme de tableaux\n",
    "import datetime as dt     # Pour convertir la date au bon format\n",
    "import os\n",
    "import sys\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer la variable de configuration pour accéder aux requêtes vers reddit\n",
    "reddit = praw.Reddit(client_id='BEjar6X3GYV5Vw', \\\n",
    "                     client_secret='lg6D6DXG14FH4kHtGVt7cins5OY', \\\n",
    "                     user_agent='feelcrypt', \\\n",
    "                     username='feelcrypt', \\\n",
    "                     password='spiderminute38')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Générer la liste des posts à traiter\n",
    "\n",
    "Cette partie va récupérer les 500 top posts du subreddit défini et sauvegarder uniquement les id de ces posts dans une liste de post à traiter (un fichier txt)\n",
    "Nous créons égalemenet tous les dossiers et fichiers nécessaires au bon fonctionnement du scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables de configuration globale\n",
    "nb_top_posts = 2  # nombre de posts selectionnés parmi les premiers (limite = 500)\n",
    "subreddit_title = 'btc' # Définir le titre du subreddit que l'on va cibler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_script_dir_path():\n",
    "    script_dir_path = %pwd \n",
    "    script_dir_path += '\\\\'\n",
    "    return script_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier si le dossier passé en paramètre (+chemin relatif au dossier du subreddit) existe, sinon le créer\n",
    "def create_folder(subreddit_path, folder_name):\n",
    "    folder_name = folder_name\n",
    "    folder_path = subreddit_path + folder_name + '\\\\'\n",
    "    try:\n",
    "        os.makedirs(folder_path)\n",
    "        print('Dossier créé : ' + folder_name) # Log\n",
    "    except FileExistsError:\n",
    "        print('Dossier déjà existant : ' + folder_name) # Log\n",
    "        pass\n",
    "    return folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer la liste des id des posts à traiter\n",
    "def create_id_todo(subreddit_path):\n",
    "    # Créer la variable sur le subreddit\n",
    "    subreddit = reddit.subreddit(subreddit_title)\n",
    "    \n",
    "    # Récupérer les n premiers posts de notre subreddit de notre liste\n",
    "    top_posts = []\n",
    "\n",
    "    for post in subreddit.top(limit = nb_top_posts):\n",
    "        top_posts.append(post)\n",
    "\n",
    "    # Enregistrer les id dans une liste\n",
    "    top_posts_id = []\n",
    "    for post in top_posts:\n",
    "        top_posts_id.append(post.id)\n",
    "\n",
    "    # Enregistrer la liste des id dans un fichier txt\n",
    "\n",
    "    # Créer le chemin de la liste d'id à traiter\n",
    "    file_name = 'id_todo.txt'\n",
    "    id_todo__path = subreddit_path + file_name\n",
    "\n",
    "    # Remplir la liste avec les id\n",
    "    top_posts_id_file = open(id_todo__path,'w')\n",
    "    for post_id in top_posts_id:\n",
    "         top_posts_id_file.write(post_id)\n",
    "         top_posts_id_file.write('\\n')\n",
    "    top_posts_id_file.close()\n",
    "    \n",
    "    # Log\n",
    "    print('Fichier initialisé : ' + file_name)\n",
    "    \n",
    "    return id_todo__path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le fichier pour compter le nombre de commentaires total\n",
    "# Le créer et l'initialiser à 0 (ecrase et remplace par 0 si le fichier existe déjà)\n",
    "def create_counter(subreddit_path):\n",
    "    file_name = 'total_comments_counter.txt'\n",
    "    counter__path = subreddit_path + file_name\n",
    "    nb_file = open(counter__path,'w').close()\n",
    "    nb_file = open(counter__path,'w')\n",
    "    nb_file.write(str(0))\n",
    "    nb_file.close()\n",
    "    \n",
    "    # log\n",
    "    print('Fichier initialisé : ' + file_name)\n",
    "    \n",
    "    return counter__path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def create_csv(subrredit_path, file_name):\n",
    "    # assemble path\n",
    "    full_path = subreddit_path + file_name\n",
    "    # Check if it exists\n",
    "    csv_exists = os.path.exists(full_path.replace(os.sep, '/'))\n",
    "    \n",
    "    if (not csv_exists):\n",
    "        print(file_name + \" does not exists >> initializing empty csv\")\n",
    "        with open(full_path, \"w\") as my_empty_csv:\n",
    "            pass\n",
    "    else:\n",
    "        print(file_name + \" already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dossier créé : btc\n",
      "Dossier créé : comments\n",
      "Dossier créé : comments_manager_id\n",
      "Dossier créé : comments_manager_counter\n"
     ]
    }
   ],
   "source": [
    "# Récupérer le chemin du dossier contenant le script python\n",
    "script_folder = get_script_dir_path()\n",
    "\n",
    "# Créer un dossier pour le subreddit\n",
    "subreddit_path = create_folder(script_folder, subreddit_title)\n",
    "\n",
    "# Vérifier si le dossier comments (chemin relatif) existe, sinon le créer\n",
    "comments_path = create_folder(subreddit_path, 'comments')\n",
    "\n",
    "# Vérifier si le dossier comments_manager_id (chemin relatif) existe, sinon le créer\n",
    "comments_manager_id_path = create_folder(subreddit_path, 'comments_manager_id')\n",
    "\n",
    "# Vérifier si le dossier comments_manager_counter (chemin relatif) existe, sinon le créer\n",
    "comments_manager_counter_path = create_folder(subreddit_path, 'comments_manager_counter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier initialisé : id_todo.txt\n"
     ]
    }
   ],
   "source": [
    "# Créer la liste des id des posts à traiter\n",
    "id_todo_path = create_id_todo(subreddit_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier initialisé : total_comments_counter.txt\n"
     ]
    }
   ],
   "source": [
    "# Créer le fichier pour compter le nombre de commentaires total\n",
    "file_name = 'total_comments_counter.txt'\n",
    "counter_path = subreddit_path + file_name\n",
    "    \n",
    "if not os.path.isfile(counter_path):\n",
    "    counter_path = create_counter(subreddit_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comments_manager_counter.csv does not exists >> initializing empty csv\n"
     ]
    }
   ],
   "source": [
    "# Créer le compteur de commentaires par post, retourné par l'api (dans un fichier csv)\n",
    "create_csv(subreddit_path, 'comments_manager_counter.csv')\n",
    "comments_manager_counter_path = subreddit_path + 'comments_manager_counter.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Traiter les posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales\n",
    "\n",
    "scrap_all_comments = False\n",
    "# Si false : prend seulement les 32 premiers commentaires de chaque post\n",
    "# Si true : prend tous les commentaires du post\n",
    "\n",
    "scrap_responses_to_comments = False\n",
    "# Si false : ne prend que les réponses directes au post\n",
    "# Si true : prend également en compte les réponses aux commentaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de conversion pour la date avec timestamp\n",
    "def get_date(created):\n",
    "    return dt.date.fromtimestamp(created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter le nb de commentaires traités dans le compteur\n",
    "def updating_counter(counter_path, nb_comments, current_post_id):\n",
    "        nb_file = open(counter_path,'r')\n",
    "        total_comments = int(nb_file.read())\n",
    "        print(\"previously total number of comments : \" + str (total_comments))\n",
    "        total_comments += nb_comments\n",
    "        nb_file.close()\n",
    "        nb_file = open(counter_path,'w').close()\n",
    "        nb_file = open(counter_path,'w')\n",
    "        nb_file.write(str(total_comments))\n",
    "        nb_file.close()\n",
    "        \n",
    "        print(str(nb_comments) + \" new comments scrapped from post \" + str(current_post_id))\n",
    "        print('total comments saved = ' + str(total_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter le nombre de lignes dans un fichier\n",
    "def file_len(fname):\n",
    "    i = 0\n",
    "    with open(fname) as f:\n",
    "        for l in enumerate(f):\n",
    "            i += 1\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter le nombre de commentaires déjà scrapés pour un post\n",
    "#def count_already_scrapped_comments(post_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer la liste des commentaires déjà traités depuis le comment_manager\n",
    "# Et comparer avec la liste récupérée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traiter chaque id du fichier todo\n",
    "def get_comments(id_todo):\n",
    "    while id_todo:  # Vérifier si la liste n'est pas vide\n",
    "\n",
    "        # Récupérer le post\n",
    "        post = reddit.submission(id = id_todo[0])\n",
    "        current_post_id = post.id[0:6]\n",
    "        print('-----------------------------------\\n\\n')\n",
    "        print('Starting working on post : ' + str(current_post_id))\n",
    "        \n",
    "        # initialiser le fichier txt pour les commentaires déjà traités, (ne change rien si le fichier existe déjà)\n",
    "        print(\"Step 1 ---- initialise txt file for done comments\")\n",
    "        manager_file_path = comments_manager_id_path + str(current_post_id) + '.txt'\n",
    "        comment_manager_file = open(manager_file_path,'a')\n",
    "        comment_manager_file.close()\n",
    "        \n",
    "        ## Comparer le nb de commentaire actuel du pos à celui enregistré\n",
    "        print(\"Step 2 ---- compare comments number on post\")\n",
    "        #nb_comments_done = file_len(manager_file_path) # old version\n",
    "        current_nb_comments = post.num_comments # nb of comments right now on the post\n",
    "        print(\"    Current number of comments on post : \" + str(current_nb_comments))\n",
    "        \n",
    "        df_counter = pd.read_csv(comments_manager_counter_path, sep=';', names =['id','counter'])\n",
    "        matching_line = df_counter[df_counter['id'].str.match(post.id)]\n",
    "        id_exist = not matching_line.empty\n",
    "        \n",
    "        to_scrap = True # boolean to know if we have to scrap this post\n",
    "        post_already_scrapped = False\n",
    "        \n",
    "        if(id_exist):\n",
    "            print('    This post was scrapped before')\n",
    "            \n",
    "            # index of matching line \n",
    "            line_index = matching_line.index[0]\n",
    "            \n",
    "            # saved counter\n",
    "            saved_counter = df_counter['counter'][line_index]\n",
    "            print(\"    Saved counter of comments on post : \" + str(saved_counter))\n",
    "            \n",
    "            # compare saved and current counter\n",
    "            to_scrap = not (saved_counter == current_nb_comments)\n",
    "\n",
    "            # replacing counter with current counter\n",
    "            df_counter.at[line_index, 'counter'] = current_nb_comments\n",
    "            \n",
    "            post_already_scrapped = True\n",
    "            \n",
    "        else: # if we could not find the id in the csv\n",
    "            # add the id and counter to csv\n",
    "            df_counter = df_counter.append({'id':current_post_id,'counter':current_nb_comments}, ignore_index=True)\n",
    "        \n",
    "            # change booleans\n",
    "            to_scrap = True\n",
    "            post_already_scrapped = False\n",
    "            print('    This post was not scrapped before')\n",
    "        # Saving the new counter csv    \n",
    "        df_counter.to_csv(comments_manager_counter_path, ';', mode='w', index=False, header=False) \n",
    "        \n",
    "        print('    Do we have to scrap this post ? : ' + str(to_scrap))\n",
    "        \n",
    "        \n",
    "        # If there are new comments to scrap\n",
    "        if to_scrap:\n",
    "            \n",
    "            ## Récupérer les commentaires du poste\n",
    "        \n",
    "            #Réinitialiser le dictionnaire\n",
    "            comments_dict = { \"created_utc\":[], \\\n",
    "                             \"body\":[], \\\n",
    "                             \"score\":[], \\\n",
    "                             \"nb_replies\":[], \\\n",
    "                             \"stickied\":[], \\\n",
    "                             \"author\":[], \\\n",
    "                             \"id\":[], \\\n",
    "                             \"post_title\":[], \\\n",
    "                             \"post_id\":[], \\\n",
    "                             \"post_link\":[]} \n",
    "\n",
    "\n",
    "            print(\"Step 4 ---- Se débarasser récursivement de la limite de 32 commentaires par requête\")\n",
    "            # Se débarasser récursivement de la limite de 32 commentaires par requête\n",
    "            # NB : Cette partie est la plus longue lorsqu'on scrap un post\n",
    "            if scrap_all_comments:\n",
    "                post.comments.replace_more(limit=None)  # prendre en compte les commentaires supp\n",
    "            else:\n",
    "                post.comments.replace_more(limit=0)    # ignorer les commentaires supp\n",
    "\n",
    "            print(\"Step 5 ---- Récupérer la liste des commentaires\")\n",
    "            # Récupérer la liste des commentaires\n",
    "            if scrap_responses_to_comments:\n",
    "                comments_list = post.comments.list()\n",
    "            else:\n",
    "                comments_list = post.comments\n",
    "\n",
    "            # Remplir le dictionnaire de commentaires avec seulement les nouveaux commentaires\n",
    "            # On check donc si le commentaire n'est pas dans la liste comment_manager_file\n",
    "            # Réouvrir le fichier des commentaires          \n",
    "            new_comments_list = []\n",
    "                      \n",
    "            for comment in comments_list:\n",
    "                bool_comment_scrapped = False\n",
    "                \n",
    "                comment_manager_file = open(manager_file_path,'r')\n",
    "                for comment_id_done in comment_manager_file:\n",
    "                    if(str(comment.id) == str(comment_id_done[0:7])):\n",
    "                        bool_comment_scrapped = True\n",
    "                comment_manager_file.close()\n",
    "                if not bool_comment_scrapped: # Si le commentaire n'avait pas été scrappé, le rajouter à la liste des commentaires scrappés\n",
    "                    new_comments_list.append(comment)\n",
    "                \n",
    "            comment_manager_file = open(manager_file_path,'a')    \n",
    "            for comment in new_comments_list:\n",
    "                #print(\"--- this is a comment id from the new_comment_list : \" + str(comment.id))\n",
    "                origin_post = comment.submission\n",
    "                comments_dict[\"created_utc\"].append(comment.created_utc)\n",
    "                comments_dict[\"body\"].append(comment.body)\n",
    "                comments_dict[\"score\"].append(comment.score)\n",
    "                comments_dict[\"nb_replies\"].append(len(comment.replies))\n",
    "                comments_dict[\"stickied\"].append(comment.stickied)\n",
    "                comments_dict[\"author\"].append(comment.author)\n",
    "                comments_dict[\"id\"].append(comment.id)\n",
    "                comments_dict[\"post_title\"].append(origin_post.title)\n",
    "                comments_dict[\"post_id\"].append(origin_post.id)\n",
    "                comments_dict[\"post_link\"].append(origin_post.url)\n",
    "\n",
    "                # Ajouter l'id du commentaires à la liste des commentaires scrappés\n",
    "                comment_manager_file.write(comment.id + '\\n')\n",
    "\n",
    "            # mettre la data au format pandas (qui permet de faire un \"tableur\" à partir du dictionnaire)\n",
    "            comments_data = pd.DataFrame(comments_dict)\n",
    "\n",
    "            # Créer la liste des dates converties et la sauvgarder dans la variable _timstamp\n",
    "            # created_utc est la colonne contenant les dates au mauvais format\n",
    "            _timestamp = comments_data[\"created_utc\"].apply(get_date)\n",
    "\n",
    "            # ajouter la liste à une nouvelle colonne appelée timestamp\n",
    "            comments_data = comments_data.assign(date = _timestamp)\n",
    "\n",
    "            # Supprimer la colonne du temps inutile\n",
    "            comments_data = comments_data.drop(columns=\"created_utc\")\n",
    "\n",
    "            # Déplacer la date en première position\n",
    "            colonnes = comments_data.columns.tolist()\n",
    "            colonnes = colonnes[-1:] + colonnes[:-1]\n",
    "            comments_data = comments_data[colonnes]\n",
    "\n",
    "            # Trier par la colonne date\n",
    "            comments_sorted = comments_data.sort_values(by=['date'])\n",
    "\n",
    "            ## Extraire plusieurs dataframe qui représentent chacun une date avec tous les commentaires dedans\n",
    "            # Récupérer la liste des dates et indexer par dates\n",
    "            comments_sorted.set_index(keys=['date'], drop=False,inplace=True)\n",
    "            dates = comments_sorted['date'].unique().tolist()\n",
    "\n",
    "            # Enregistrer dans une liste contenant chaque dataframe (1 data frame = 1 date)\n",
    "            comments_splitperday = []\n",
    "            for date in dates:\n",
    "                comments_per_day = pd.DataFrame(comments_sorted.loc[comments_sorted.date == date])\n",
    "                comments_splitperday.append(comments_per_day)\n",
    "\n",
    "            # Enregistrer chaque dataframe dans un fichier csv\n",
    "            for dataframe in comments_splitperday:\n",
    "                # Récupérer la date du dataframe supprimer la colonne date\n",
    "                date = str(dataframe.date.iloc[0])\n",
    "                dataframe = dataframe.drop(columns=\"date\")\n",
    "                csv_path = comments_path + date + '.csv'\n",
    "\n",
    "                # Check if file is empty (used to sed header or not)\n",
    "                csv_exists = os.path.exists(csv_path.replace(os.sep, '/'))\n",
    "\n",
    "                # Enregistrer au format csv avec pour nom la date\n",
    "                dataframe.to_csv(csv_path, ';', mode='a', index=False, header= not csv_exists) \n",
    "                \n",
    "                #----- Fin du traitement des commentaires d'un post d'un post\n",
    "\n",
    "            # Retirer le premier id de la liste, le laisser à la fin, si le while est interrompu, il sera retiré alors que le post n'aura pas été traité\n",
    "            id_todo.pop(0)\n",
    "\n",
    "            # Ajouter l'id du post lu dans la liste des id_done si le post n'avait pas déjà été scrappé\n",
    "            if (post_already_scrapped):\n",
    "                id_done_path = subreddit_path + \"id_done.txt\"\n",
    "                with open(id_done_path, \"a\") as file:\n",
    "                    file.write(current_post_id)\n",
    "\n",
    "            # Enregistrer la nouvelle liste des id_todo dans le fichier txt (ou le créer s'il n'existe pas encore)\n",
    "            with open(id_todo_path, \"r\") as file:\n",
    "                data = file.read()\n",
    "            with open(id_todo_path, \"w\") as file:\n",
    "                for post_id in id_todo:\n",
    "                    file.write(post_id)\n",
    "\n",
    "            # Ajouter le nb de commentaires traités dans le compteur\n",
    "            updating_counter(counter_path, len(comments_data.index), current_post_id)\n",
    "\n",
    "            # Fermer le fichier comment_manager\n",
    "            comment_manager_file.close()\n",
    "            \n",
    "        else:\n",
    "            # Retirer le premier id de la liste, le laisser à la fin, si le while est interrompu, il sera retiré alors que le post n'aura pas été traité\n",
    "            id_todo.pop(0)\n",
    "\n",
    "            # Enregistrer la nouvelle liste des id_todo dans le fichier txt (ou le créer s'il n'existe pas encore)\n",
    "            with open(id_todo_path, \"r\") as file:\n",
    "                data = file.read()\n",
    "            with open(id_todo_path, \"w\") as file:\n",
    "                for post_id in id_todo:\n",
    "                    file.write(post_id)\n",
    "                    \n",
    "            # Fermer le fichier comment_manager\n",
    "            comment_manager_file.close()\n",
    "            \n",
    "    else: # when list is empty\n",
    "        print('')\n",
    "        print('============================================')\n",
    "        print('============================================')\n",
    "        print('')\n",
    "        print(\"La liste est vide, tout a été traité\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checker quels posts ont de nouveaux commentaires et les extraire\n",
    "#id_todo est une liste des id a faire, et non le fichier.txt.\n",
    "def check_new_comments_posts(id_todo):\n",
    "    # ouvrir le csv, ou le créer\n",
    "            \n",
    "    # Check if file is empty (used to sed header or not)\n",
    "    csv_exists = os.path.exists(csv_path.replace(os.sep, '/'))\n",
    "\n",
    "    # Enregistrer au format csv avec pour nom la date\n",
    "    dataframe.to_csv(csv_path, ';', mode='a', index=False, header= not csv_exists) \n",
    "    \n",
    "    while id_todo:  # Vérifier si la liste n'est pas vide\n",
    "        post.id = id_todo[0]\n",
    "        id_todo.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport pandas as pd\\npath_to_manager = 'C:\\\\Users\\\\Louis\\\\feelcrypt\\\\FeelCrypt\\\\scrapping_reddit\\\\scrapping_V5\\\\btc\\\\comments_manager_counter.csv'\\n\\ncomments_manager_counter = {'id':['abcdef', 'ghijkl'], 'counter':['55','200']}\\ndataframe = pd.DataFrame(comments_manager_counter)\\n\\ndataframe.to_csv(path_to_manager, ';', mode='w', index=False, header= False) \\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test writing a pandas to csv\n",
    "'''\n",
    "import pandas as pd\n",
    "path_to_manager = 'C:\\\\Users\\\\Louis\\\\feelcrypt\\\\FeelCrypt\\\\scrapping_reddit\\\\scrapping_V5\\\\btc\\\\comments_manager_counter.csv'\n",
    "\n",
    "comments_manager_counter = {'id':['abcdef', 'ghijkl'], 'counter':['55','200']}\n",
    "dataframe = pd.DataFrame(comments_manager_counter)\n",
    "\n",
    "dataframe.to_csv(path_to_manager, ';', mode='w', index=False, header= False) \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv exists : True\n"
     ]
    }
   ],
   "source": [
    "# Test if csv exists\n",
    "path_to_manager = 'C:\\\\Users\\\\Louis\\\\feelcrypt\\\\FeelCrypt\\\\scrapping_reddit\\\\scrapping_V5\\\\btc\\\\comments_manager_counter.csv'\n",
    "counter_csv_exists = os.path.exists(path_to_manager.replace(os.sep, '/'))\n",
    "print(\"csv exists : \" + str(counter_csv_exists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3) Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ouvrir la liste de posts pas encore faits\n",
    "with open(id_todo_path) as file:\n",
    "  id_todo = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "\n",
      "\n",
      "Starting working on post : 7eil12\n",
      "Step 1 ---- initialise txt file for done comments\n",
      "Step 2 ---- compare comments number on post\n",
      "    Current number of comments on post : 1162\n",
      "    This post was not scrapped before\n",
      "    Do we have to scrap this post ? : True\n",
      "Step 4 ---- Se débarasser récursivement de la limite de 32 commentaires par requête\n",
      "Step 5 ---- Récupérer la liste des commentaires\n",
      "previously total number of comments : 0\n",
      "167 new comments scrapped from post 7eil12\n",
      "total comments saved = 167\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "Starting working on post : 7opi7w\n",
      "Step 1 ---- initialise txt file for done comments\n",
      "Step 2 ---- compare comments number on post\n",
      "    Current number of comments on post : 747\n",
      "    This post was not scrapped before\n",
      "    Do we have to scrap this post ? : True\n",
      "Step 4 ---- Se débarasser récursivement de la limite de 32 commentaires par requête\n",
      "Step 5 ---- Récupérer la liste des commentaires\n",
      "previously total number of comments : 167\n",
      "119 new comments scrapped from post 7opi7w\n",
      "total comments saved = 286\n",
      "\n",
      "============================================\n",
      "============================================\n",
      "\n",
      "La liste est vide, tout a été traité\n"
     ]
    }
   ],
   "source": [
    "# Traiter chaque id du fichier todo\n",
    "get_comments(id_todo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Supprimer toute la data\n",
    "utile uniquement en debbug, permet de supprimer tous les dossiers générés et leur contenu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Delete_all_data():\n",
    "    if os.path.isdir(subreddit_path):\n",
    "        shutil.rmtree(subreddit_path)\n",
    "        print('Dossier supprimé : ' + subreddit_path)\n",
    "    else:\n",
    "        print('Dossier inexistant : ' + subreddit_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dossier supprimé : C:\\Users\\Louis\\feelcrypt\\FeelCrypt\\scrapping_reddit\\scrapping_V5\\btc\\\n"
     ]
    }
   ],
   "source": [
    "#Delete_all_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
