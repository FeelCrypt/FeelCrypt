{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Configuration de PRAW (Python Reddit Api Wrapper)\n",
    "## !! A faire avant de pouvoir utiliser n'importe laquelle des parties suivantes !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw               # API pour reddit\n",
    "import pandas as pd       # pour afficher les dictionnaires sous forme de tableaux\n",
    "import datetime as dt     # Pour convertir la date au bon format\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer la variable de configuration pour accéder aux requêtes vers reddit\n",
    "reddit = praw.Reddit(client_id='BEjar6X3GYV5Vw', \\\n",
    "                     client_secret='lg6D6DXG14FH4kHtGVt7cins5OY', \\\n",
    "                     user_agent='feelcrypt', \\\n",
    "                     username='feelcrypt', \\\n",
    "                     password='spiderminute38')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Générer la liste des posts à traiter\n",
    "## !! Ne lancer qu'une fois au totale !!\n",
    "\n",
    "Cette partie va récupérer les 500 top posts du subreddit défini et sauvegarder uniquement les id de ces posts dans une liste de post à traiter (un fichier txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables de configuration globale\n",
    "nb_top_posts = 5  # nombre de posts selectionnés parmi les premiers (limite = 500)\n",
    "subreddit_title = 'bitcoin' # Définir le titre du subreddit que l'on va cibler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dossier C:\\Users\\Louis\\feelcrypt\\FeelCrypt\\scrapping_reddit\\scrapping_V2\\bitcoin\\ créé\n"
     ]
    }
   ],
   "source": [
    "# Créer un dossier pour le subreddit\n",
    "\n",
    "# Générer le chemin en fonction du subreddit traité\n",
    "script_dir = %pwd\n",
    "subreddit_dir = script_dir + '\\\\'+ subreddit_title +'\\\\' #\n",
    "try:\n",
    "    os.makedirs(subreddit_dir)\n",
    "    print('Dossier ' + subreddit_dir + ' créé')\n",
    "except FileExistsError:\n",
    "    print('Dossier déjà existant')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dossier C:\\Users\\Louis\\feelcrypt\\FeelCrypt\\scrapping_reddit\\scrapping_V2\\bitcoin\\comments créé\n"
     ]
    }
   ],
   "source": [
    "# Vérifier si le dossier comments (chemin relatif) existe, sinon le créer\n",
    "comments_dir = subreddit_dir + 'comments'\n",
    "try:\n",
    "    os.makedirs(comments_dir)\n",
    "    print('Dossier ' + comments_dir + ' créé')\n",
    "except FileExistsError:\n",
    "    print('Dossier déjà existant')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer la variable sur le subreddit btc\n",
    "subreddit = reddit.subreddit(subreddit_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer les n premiers posts de notre subreddit de notre liste\n",
    "top_posts = []\n",
    "\n",
    "for post in subreddit.top(limit = nb_top_posts):\n",
    "    top_posts.append(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrer les id dans une liste\n",
    "top_posts_id = []\n",
    "for post in top_posts:\n",
    "    top_posts_id.append(post.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrer la liste des id dans un fichier txt\n",
    "id_todo_path = subreddit_dir +'id_todo.txt'\n",
    "top_posts_id_file = open(id_todo_path,'w')\n",
    "for post_id in top_posts_id:\n",
    "     top_posts_id_file.write(post_id)\n",
    "     top_posts_id_file.write('\\n')\n",
    "top_posts_id_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fichier pour compter le nombre de commentaires total\n",
    "# Le créer et le réinitialiser à 0 s'il existe déjà\n",
    "nb_file = open('comments_counter.txt','w').close()\n",
    "nb_file = open('comments_counter.txt','w')\n",
    "nb_file.write(str(0))\n",
    "nb_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2) Traiter les top posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "\n",
    "scrap_all_comments = False\n",
    "# Si false : prend seulement les 32 premiers commentaires de chaque post\n",
    "# Si true : prend tous les commentaires du post\n",
    "\n",
    "scrap_responses_to_comments = True\n",
    "# Si false : ne prend que les réponses directes au post\n",
    "# Si true : prend également en compte les réponses aux commentaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de conversion pour la date avec timestamp\n",
    "def get_date(created):\n",
    "    return dt.date.fromtimestamp(created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ouvrir la liste de posts pas encore faits\n",
    "with open(id_todo_path) as file:\n",
    "  id_todo = file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ne pas oublier de remettre le replace_more limit = NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Starting working on post : 7g9cd3\n",
      "\n",
      "493 comments from post 7g9cd3\n",
      "\n",
      "total comments saved = 493\n",
      "-----------------------------------\n",
      "Starting working on post : 7v438b\n",
      "\n",
      "440 comments from post 7v438b\n",
      "\n",
      "total comments saved = 933\n",
      "-----------------------------------\n",
      "Starting working on post : 7fkqh5\n",
      "\n",
      "479 comments from post 7fkqh5\n",
      "\n",
      "total comments saved = 1412\n",
      "-----------------------------------\n",
      "Starting working on post : 7olruz\n",
      "\n",
      "453 comments from post 7olruz\n",
      "\n",
      "total comments saved = 1865\n",
      "-----------------------------------\n",
      "Starting working on post : chbx9d\n",
      "\n",
      "468 comments from post chbx9d\n",
      "\n",
      "total comments saved = 2333\n",
      "-----------------------------------\n",
      "Starting working on post : 7gla05\n",
      "\n",
      "478 comments from post 7gla05\n",
      "\n",
      "total comments saved = 2811\n",
      "-----------------------------------\n",
      "Starting working on post : 7u9zbd\n",
      "\n",
      "416 comments from post 7u9zbd\n",
      "\n",
      "total comments saved = 3227\n",
      "-----------------------------------\n",
      "Starting working on post : 7jht7c\n",
      "\n",
      "469 comments from post 7jht7c\n",
      "\n",
      "total comments saved = 3696\n",
      "-----------------------------------\n",
      "Starting working on post : 7dpypn\n",
      "\n",
      "466 comments from post 7dpypn\n",
      "\n",
      "total comments saved = 4162\n",
      "-----------------------------------\n",
      "Starting working on post : a44t1m\n",
      "\n",
      "353 comments from post a44t1m\n",
      "\n",
      "total comments saved = 4515\n",
      "-----------------------------------\n",
      "Starting working on post : 7yaaz2\n",
      "\n",
      "470 comments from post 7yaaz2\n",
      "\n",
      "total comments saved = 4985\n",
      "-----------------------------------\n",
      "Starting working on post : 7j584c\n",
      "\n",
      "333 comments from post 7j584c\n",
      "\n",
      "total comments saved = 5318\n",
      "-----------------------------------\n",
      "Starting working on post : 5y0e33\n",
      "\n",
      "489 comments from post 5y0e33\n",
      "\n",
      "total comments saved = 5807\n",
      "-----------------------------------\n",
      "Starting working on post : 8982l3\n",
      "\n",
      "471 comments from post 8982l3\n",
      "\n",
      "total comments saved = 6278\n",
      "-----------------------------------\n",
      "Starting working on post : 8d3ziy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Traiter chaque id du fichier todo\n",
    "\n",
    "while id_todo:  # Vérifier si la liste n'est pas vide\n",
    "    \n",
    "    # Récupérer le post\n",
    "    post = reddit.submission(id = id_todo[0])\n",
    "    current_post_id = post.id\n",
    "    print('-----------------------------------')\n",
    "    print('Starting working on post : ' + str(current_post_id))\n",
    "    \n",
    "    ## Récupérer les commentaires du poste\n",
    "    comments_dict = { \"created_utc\":[], \"body\":[]} #Réinitialiser le dictionnaire\n",
    "    \n",
    "    #se débarasser récursivement de la limite de 32 commentaires par requête\n",
    "    if scrap_all_comments:\n",
    "        post.comments.replace_more(limit=None)  # prendre en compte les commentaires supp\n",
    "    else:\n",
    "        post.comments.replace_more(limit=0)    # ignorer les commentaires supp\n",
    "    \n",
    "    # Récupérer la liste des commentaires\n",
    "    if scrap_responses_to_comments:\n",
    "        comments_list = post.comments.list()\n",
    "    else:\n",
    "        comments_list = post.comments\n",
    "        \n",
    "    # Remplir le dictionnaire de commentaires avec la liste  \n",
    "    for comment in comments_list:\n",
    "        comments_dict[\"created_utc\"].append(comment.created_utc)\n",
    "        comments_dict[\"body\"].append(comment.body)\n",
    "        \n",
    "    # mettre la data au format pandas (qui permet de faire un \"tableur\" à partir du dictionnaire)\n",
    "    comments_data = pd.DataFrame(comments_dict)\n",
    "    \n",
    "    # Créer la liste des dates converties et la sauvgarder dans la variable _timstamp\n",
    "    # created_utc est la colonne contenant les dates au mauvais format\n",
    "    _timestamp = comments_data[\"created_utc\"].apply(get_date)\n",
    "\n",
    "    # ajouter la liste à une nouvelle colonne appelée timestamp\n",
    "    comments_data = comments_data.assign(date = _timestamp)\n",
    "    \n",
    "    # Supprimer la colonne du temps inutile\n",
    "    comments_data = comments_data.drop(columns=\"created_utc\")\n",
    "\n",
    "    # Déplacer la date en première position\n",
    "    colonnes = comments_data.columns.tolist()\n",
    "    colonnes = colonnes[-1:] + colonnes[:-1]\n",
    "    comments_data = comments_data[colonnes]\n",
    "    \n",
    "    # Trier par la colonne date\n",
    "    comments_sorted = comments_data.sort_values(by=['date'])\n",
    "    \n",
    "    ## Extraire plusieurs dataframe qui représentent chacun une date avec tous les commentaires dedans\n",
    "    # Récupérer la liste des dates et indexer par dates\n",
    "    comments_sorted.set_index(keys=['date'], drop=False,inplace=True)\n",
    "    dates = comments_sorted['date'].unique().tolist()\n",
    "    \n",
    "    # Enregistrer dans une liste contenant chaque dataframe (1 data frame = 1 date)\n",
    "    comments_splitperday = []\n",
    "    for date in dates:\n",
    "        comments_per_day = pd.DataFrame(comments_sorted.loc[comments_sorted.date == date])\n",
    "        comments_splitperday.append(comments_per_day)\n",
    "    \n",
    "    # Enregistrer chaque dataframe dans un fichier csv\n",
    "    for dataframe in comments_splitperday:\n",
    "        # Récupérer la date du dataframe supprimer la colonne date\n",
    "        date = str(dataframe.date.iloc[0])\n",
    "        dataframe = dataframe.drop(columns=\"date\")\n",
    "        csv_path = comments_dir + \"\\\\\" + date + '.csv'\n",
    "        \n",
    "        # Enregistrer au format csv avec pour nom la date\n",
    "        dataframe.to_csv(csv_path, ';', mode='a', index=False, header=False) \n",
    "        \n",
    "    # Retirer le premier id de la liste, le laisser à la fin, si le while est interrompu, il sera retiré alors que le post n'aura pas été traité\n",
    "    id_todo.pop(0)\n",
    "    \n",
    "    # Ajouter l'id du post lu dans la liste des id_done\n",
    "    with open(subreddit_dir + \"top_posts_id_done.txt\", \"a\") as file:\n",
    "        file.write(current_post_id)\n",
    "       \n",
    "    # Enregistrer la nouvelle liste des id_todo dans le fichier txt (ou le créer s'il n'existe pas encore)\n",
    "    with open(id_todo_path, \"r\") as file:\n",
    "        data = file.read()\n",
    "    with open(id_todo_path, \"w\") as file:\n",
    "        for post_id in id_todo:\n",
    "            file.write(post_id)\n",
    "    \n",
    "    # Ajouter le nb de commentaires traités dans le compteur\n",
    "    nb_file = open('comments_counter.txt','r')\n",
    "    total_comments = int(nb_file.read())\n",
    "    total_comments += len(comments_data.index)\n",
    "    nb_file.close()\n",
    "    nb_file = open('comments_counter.txt','w').close()\n",
    "    nb_file = open('comments_counter.txt','w')\n",
    "    nb_file.write(str(total_comments))\n",
    "    nb_file.close()\n",
    "    \n",
    "    print(str(len(comments_data.index)) + \" comments from post \" + str(current_post_id))\n",
    "    print('total comments saved = ' + str(total_comments))\n",
    "else:\n",
    "    print('---------------------------------------------------------')\n",
    "    print(\"La liste est vide, tout a été traité\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'CommentHelper' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-d25ca46042f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubreddit_btc_6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'CommentHelper' has no len()"
     ]
    }
   ],
   "source": [
    "len(subreddit.comments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
